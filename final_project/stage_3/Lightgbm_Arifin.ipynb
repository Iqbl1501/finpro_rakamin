{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F3ykbS_3KId"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVzb0Anq3KIl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wfmJsolo3KIo",
        "outputId": "5129d7c3-a1f1-4134-9e4a-ec31cb21674f"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../dataset/after_selection/train.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Data Train & Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdZuMeI3KIq"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['Response'])\n",
        "y = df[['Response']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algoritma Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "\n",
        "\n",
        "def eval_classification(model, X_train, X_test, y_train, y_test, n_splits=5):\n",
        "    # Evaluate on the test set\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # StratifiedKFold for cross-validation with stratified sampling\n",
        "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    cv_test_results = cross_validate(model, X_test, y_test, scoring=['roc_auc'],\n",
        "                                cv=cv, return_train_score=False)\n",
        "    cv_train_results = cross_validate(model, X_train, y_train, scoring=['roc_auc'],\n",
        "                                cv=cv, return_train_score=False)\n",
        "    \n",
        "\n",
        "    # Display metrics for the training set\n",
        "    print(\"Metrics for the Test Set:\")\n",
        "    print(\"Accuracy: %.2f\" % accuracy_score(y_test, y_pred_test))\n",
        "    print(\"Precision: %.2f\" % precision_score(y_test, y_pred_test))\n",
        "    print(\"Recall: %.2f\" % recall_score(y_test, y_pred_test))\n",
        "    print(\"F1-Score: %.2f\" % f1_score(y_test, y_pred_test))\n",
        "    print()\n",
        "\n",
        "    # Display cross-validation results\n",
        "    print(\"Metrics Using Cross Validation:\")\n",
        "    print(f\"Mean ROC-AUC (Test): {cv_test_results['test_roc_auc'].mean():.2f}\")\n",
        "    print(f\"Std ROC-AUC (Test): {cv_test_results['test_roc_auc'].std():.2f}\")\n",
        "    print()\n",
        "    print(f\"Mean ROC-AUC (Train): {cv_train_results['test_roc_auc'].mean():.2f}\")\n",
        "    print(f\"Std ROC-AUC (Train): {cv_train_results['test_roc_auc'].std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lightgbm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = LGBMClassifier()\n",
        "base_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_classification(base_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgb_model = LGBMClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'boosting_type': ['gbdt', 'dart', 'goss'],\n",
        "    'num_leaves': [20, 31, 40, 50],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'subsample_for_bin': [20000, 30000, 40000],\n",
        "    'min_child_samples': [10, 20, 30],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "    'reg_lambda': [0, 0.1, 0.5, 1],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'max_depth': [-1, 6, 8, 10, 12],\n",
        "    'class_weight': ['balanced', None],\n",
        "    'metric': ['binary_logloss', 'auc'],\n",
        "    'is_unbalance': [True, False],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=lgb_model, param_grid=param_grid, scoring='roc_auc', cv=3, verbose=1)\n",
        "\n",
        "# Perform grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the best model from grid search\n",
        "best_lgbm_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_classification(best_lgbm_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filename = './models/lgbm_model.sav'\n",
        "pickle.dump(best_lgbm_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "feature_importances = best_lgbm_model.feature_importances_\n",
        "\n",
        "# Get feature names (optional)\n",
        "feature_names = [f'Feature {i}' for i in range(X.shape[1])]\n",
        "\n",
        "# Create a DataFrame for better visualization (optional)\n",
        "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)\n",
        "\n",
        "# Print or plot the feature importance\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
